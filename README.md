# VLM Chain-of-Thought Attention Analysis

A comprehensive framework for analyzing attention patterns in Vision-Language Models, with a focus on chain-of-thought reasoning.

## ğŸ“ Project Structure

```
vlm-cot-attn-analysis/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ data/                        # Image data for experiments
â”œâ”€â”€ models/                      # Model implementations
â”‚   â”œâ”€â”€ llava/                   # LLaVA model package
â”‚   â”‚   â”œâ”€â”€ config.py           # Model configuration
â”‚   â”‚   â”œâ”€â”€ model_loader.py     # Model loading utilities
â”‚   â”‚   â”œâ”€â”€ inference.py # Core inference engine
â”‚   â”‚   â”œâ”€â”€ image_processor.py  # Image preprocessing
â”‚   â”‚   â”œâ”€â”€ main.py            # CLI interface
â”‚   â”‚   â”œâ”€â”€ example_usage.py   # Basic usage examples
â”‚   â”‚   â””â”€â”€ model_weights/     # Downloaded model weights
â”‚   â””â”€â”€ utils/                  # Model utilities
â””â”€â”€ src/                        # Analysis and research code
    â”œâ”€â”€ analyzers/              # Attention analysis tools
    â”‚   â”œâ”€â”€ attention_analyzer.py  # Core attention analysis
    â”‚   â””â”€â”€ llava_analyzer.py      # Integrated LLaVA analyzer
    â”œâ”€â”€ examples/               # Usage examples
    â”‚   â”œâ”€â”€ basic_usage.py      # Basic inference examples
    â”‚   â””â”€â”€ attention_analysis.py # Advanced analysis examples
    â”œâ”€â”€ experiments/            # Research experiments
    â””â”€â”€ utils/                  # Analysis utilities
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd vlm-cot-attn-analysis

# Install dependencies
pip install -r requirements.txt
```

### 2. Basic Usage

```python
# Basic LLaVA inference
from models.llava import LLaVAInferenceEngine

engine = LLaVAInferenceEngine()
engine.load_model()
result = engine.generate_response("path/to/image.jpg", "What do you see?")
print(result['response'])
```

### 3. Advanced Attention Analysis

```python
# Comprehensive attention analysis
from src.analyzers import LLaVAAnalyzer

analyzer = LLaVAAnalyzer()
analyzer.load_model()
result = analyzer.analyze_image_response(
    image="path/to/image.jpg",
    question="What do you see?",
    analyze_attention=True
)

# Access attention analysis results
attention_data = result['attention_analysis']
print(f"Image attention summary: {attention_data['image_attention']['summary']}")
```

## ğŸ“š Usage Examples

### Run Basic Examples
```bash
# Basic inference
python src/examples/basic_usage.py

# Advanced attention analysis
python src/examples/attention_analysis.py
```

### Programmatic Usage
```python
# Run examples programmatically
from src.examples import run_basic_example, run_attention_example

run_basic_example()      # Basic inference
run_attention_example()  # Attention analysis
```

## ğŸ§  Analysis Capabilities

### Attention Analysis Features
- **Image-Text Attention Mapping**: Analyze how the model attends to image regions
- **Head Specialization Analysis**: Study attention head specialization patterns
- **Statistical Analysis**: Compute entropy, variance, and other attention statistics
- **Comparative Analysis**: Compare attention patterns across different questions
- **Batch Processing**: Analyze multiple image-question pairs efficiently

### Supported Models
- **LLaVA-1.6-Vicuna-7B**: Primary focus with comprehensive support
- **Extensible**: Framework designed for easy addition of other VLMs

## ğŸ”§ Configuration

### GPU Configuration
```python
from models.llava import Config

# Set specific GPU
Config.set_cuda_device("0")

# Or use convenience functions
from src.analyzers import create_llava_analyzer
analyzer = create_llava_analyzer(gpu_device="0")
```

### Model Configuration
```python
# Custom generation settings
config = Config()
config.GENERATION_CONFIG["max_new_tokens"] = 256
config.GENERATION_CONFIG["temperature"] = 0.1

# Use with inference engine
engine = LLaVAInferenceEngine(config)
```

## ğŸ“Š Research Applications

This framework is designed for:
- **Attention Pattern Studies**: Understanding how VLMs process visual information
- **Chain-of-Thought Analysis**: Analyzing reasoning patterns in VLM responses
- **Model Interpretability**: Making VLM decision processes more transparent
- **Comparative Studies**: Comparing different models or configurations

## ğŸ› ï¸ Development

### Adding New Analyzers
```python
# Create new analyzer in src/analyzers/
class CustomAnalyzer:
    def __init__(self, config=None):
        # Initialize analyzer
        pass
    
    def analyze_custom_pattern(self, attention_data):
        # Implement custom analysis
        pass
```

### Adding New Experiments
```python
# Create experiment in src/experiments/
def experiment_attention_evolution():
    # Implement experiment logic
    pass
```

## ğŸ“‹ Requirements

- Python 3.8+
- PyTorch 2.0+
- Transformers 4.35+
- CUDA-capable GPU (recommended)
- 16GB+ GPU memory for LLaVA-7B

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add your analysis tools to `src/analyzers/` or experiments to `src/experiments/`
4. Update documentation
5. Submit a pull request

## ğŸ“„ License

[Add your license information here]

## ğŸ™ Acknowledgments

- LLaVA team for the original model
- Hugging Face for the transformers library
- Research community for attention analysis methodologies